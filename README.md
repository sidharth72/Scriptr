# Scriptr

Scriptr is a fine-tuned Gemma 2 2B instruct model designed for generating YouTube scripts based on provided keywords. This project leverages state-of-the-art natural language processing techniques to create high-quality and relevant scripts efficiently.

## Demo

```

keywords
Deep Learning, Transformers, Neural Networks, Attention, CNN, LSTM, GPU, Parallel, Compute, Data
script
(upbeat music) Hey guys. So I've been hearing a lot about this new AI called GPT-3 which is like the next evolution of neural networks and it can generate text that almost seems human written but in reality is generated by just billions and billions of parameters within its network... 45 billion to be exact. Now you might have heard all sorts of claims around how good or bad it is but most people don't really understand what makes these models tick so today we are going to break down exactly how they work using examples from different domains: natural language processing (NLP), computer vision, image generation with a little bit of math thrown in for fun as well! And lastly, let me know if any of you would want us to build our own model based on one of your favorite applications? Alright, before diving into building something though, It's important to talk about some key terms first. When you hear about things such as "Neural Network" or "CNN", Don't expect those words to mean anything specific because there isn't even an official definition yet. But instead think of them as categories rather than names of particular architectures. For example, when talking about deep learning in general, You could say "I was doing research on convolutional neural networks." In contrast, If someone were describing their work involving NLP tasks then They may say "I'm working on recurrent neural networks". Or they might also mention LSTMs, Bidirectional RNNs, transformers, attention mechanisms etc. The main point here being that There aren't necessarily strict rules defining each term. Instead, Think of them more like labels or tags meant to categorize various types of problems or approaches taken towards solving them. With that said, Let's dive right back into analyzing both Transformer and Recurrent Neural Networks starting off with Long Short Term Memory (LSTM). This architecture has become quite popular recently due to its success at tackling sequential data such as time series predictions and speech recognition where information further along in sequence carries importance over earlier parts. Imagine yourself trying to predict the price of stock tomorrow depending solely on yesterday's closing value. Even though you didn't get much useful information from previous days, That past day still impacts current decisions; hence why having access to historical prices becomes crucial. Luckily, Unlike traditional recurrent neural networks, Which struggle remembering long sequences since information gets forgotten very quickly after every step

```

## Features
- Generates YouTube scripts from simple keyword inputs.
- Supports inference on both CPU and GPU environments.

## Getting Started

### Prerequisites
Ensure you have Python installed along with the following dependencies:
- `torch`
- `bitsandbytes`
- `transformers`
- `peft`
- `accelerate`
- `trl`

### Installation and Usage
1. Navigate to the `Scriptr` directory.
2. Install the required dependencies:
```
pip install torch bitsandbytes transformers peft accelerate trl
```
3. Run the `inference.py` file:
```
python inference.py
```
- **CPU Environment**: Expect slower performance.
- **GPU Environment**: Significantly faster inference.

## Notes
- The first run may take some time as the model initializes.
- Ensure a compatible GPU is available for optimal performance.
