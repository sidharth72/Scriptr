{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9788493,"sourceType":"datasetVersion","datasetId":5962519}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Installing required packages\n\n#### 1. `bitsandbytes`\r\n- **Purpose**: Provides 8-bit optimizers and quantization techniques to reduce memory usage and computational load.\r\n- **Usage**: \r\n  - Helps in reducing the size of large language models (LLMs) without significant loss in performance.\r\n  - Used for memory-efficient fine-tuning.\r\n\r\n#### 2. `transformers`\r\n- **Purpose**: The core library for state-of-the-art NLP models like BERT, GPT, T5, etc.\r\n- **Usage**:\r\n  - Provides pre-trained models and APIs for tokenization, model inference, and fine-tuning.\r\n  - Essential for working with transformer-based architectures.\r\n\r\n#### 3. `peft` (Parameter-Efficient Fine-Tuning)\r\n- **Purpose**: Focuses on fine-tuning methods that reduce the number of parameters that need to be updated.\r\n- **Usage**:\r\n  - Techniques like LoRA (Low-Rank Adaptation) or prefix-tuning to adapt LLMs efficiently.\r\n  - Ideal for scenarios with limited computational resources.\r\n\r\n#### 4. `accelerate`\r\n- **Purpose**: Simplifies distributed training and inference of models across multiple devices.\r\n- **Usage**:\r\n  - Facilitates seamless training on CPUs, GPUs, or TPUs.\r\n  - Helps in optimizing performance for large-scale models.\r\n\r\n#### 5. `trl` (Transformers Reinforcement Learning)\r\n- **Purpose**: Provides tools to train language models using reinforcement learning.\r\n- **Usage**:\r\n  - Implements techniques like PPO (Proximal Policy Optimization) to align models with specific goals or preferences.\r\n  - Useful for tasks where human feedback or reward signals guide the modelâ€™s training.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:46:30.912510Z","iopub.execute_input":"2024-11-09T07:46:30.912826Z","iopub.status.idle":"2024-11-09T07:47:47.774902Z","shell.execute_reply.started":"2024-11-09T07:46:30.912792Z","shell.execute_reply":"2024-11-09T07:47:47.773884Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Importing Necessary packages","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, HfArgumentParser, pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:48:17.962297Z","iopub.execute_input":"2024-11-09T07:48:17.963304Z","iopub.status.idle":"2024-11-09T07:48:19.149593Z","shell.execute_reply.started":"2024-11-09T07:48:17.963251Z","shell.execute_reply":"2024-11-09T07:48:19.148593Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"You might want to set your HuggingFace token in Kaggle Secrets if you are using Kaggle, otherwise, you can change the code to set for environment variables.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nwandb_token = user_secrets.get_secret(\"WandB_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:48:20.646546Z","iopub.execute_input":"2024-11-09T07:48:20.646940Z","iopub.status.idle":"2024-11-09T07:48:21.322789Z","shell.execute_reply.started":"2024-11-09T07:48:20.646902Z","shell.execute_reply":"2024-11-09T07:48:21.321814Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Logging into the Hugging Face Hub using the token stored in the hf_token variable is helpful for pushing the fine-tuned model to Hugging Face.","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token $hf_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:48:22.131850Z","iopub.execute_input":"2024-11-09T07:48:22.132234Z","iopub.status.idle":"2024-11-09T07:48:23.789909Z","shell.execute_reply.started":"2024-11-09T07:48:22.132193Z","shell.execute_reply":"2024-11-09T07:48:23.788466Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Configuring Weights and Biases\n\nWandB, or Weights and Biases, is a great tool for monitoring and visualizing the training process of machine learning models. It allows users to track various metrics in real-time, offering insights into key aspects such as GPU utilization, memory consumption, and loss metrics throughout the training phase.\n\nTo configure, you can create an account at https://wandb.ai/ and then grab the API token to use in the code.","metadata":{}},{"cell_type":"code","source":"wandb.login(key = wandb_token)\n\nrun = wandb.init (\n    project = \"Scripter - Gemma Finetuning\",\n    job_type = 'training',\n    anonymous = 'allow'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = \"google/gemma-2-2b-it\"\nnew_model = 'gemma2_scripter'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:48:29.195912Z","iopub.execute_input":"2024-11-09T07:48:29.196876Z","iopub.status.idle":"2024-11-09T07:48:29.201288Z","shell.execute_reply.started":"2024-11-09T07:48:29.196831Z","shell.execute_reply":"2024-11-09T07:48:29.200300Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Quantizing the Model\n\nWe are using the 4bit quantization using bitsandbytes. This will help to reduce the computation and memory cost for training.","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\nmodel.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the Tokenizer of Gemma 2 model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code = True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For training the model with LoRA, we need to add extra adapter weights to the model, for that we need to know the linear modules for attaching the Low rank adapters. The below code will help us to find all the linear modules where we can attach the extra low rank adapters.","metadata":{}},{"cell_type":"code","source":"import bitsandbytes\n\ndef find_all_linear_names(model):\n    cls = bitsandbytes.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"modules","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PEFT Configuration","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n\n    lora_alpha = 16,\n    lora_dropout = 0.1,\n    r = 16,\n    bias = 'none',\n    task_type = 'CAUSAL_LM',\n    target_modules = modules\n    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Arguments ","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir ='./results',\n    num_train_epochs = 3,\n    per_device_train_batch_size = 4,\n    gradient_accumulation_steps = 4,\n    optim = 'paged_adamw_32bit',\n    save_steps = 50,\n    logging_steps = 5,\n    learning_rate = 1e-4,\n    weight_decay = 0.01,\n    fp16 = False,\n    bf16 = False,\n    max_grad_norm = 0.3,\n    max_steps = -1,\n    warmup_ratio = 0.05,\n    group_by_length = True,\n    lr_scheduler_type = 'cosine',\n    report_to = \"wandb\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading and Preprocessing the Data for Training\n\nThe data is available on Kaggle. You can find it here: https://www.kaggle.com/datasets/sidharthangn/youtube-subtitles-and-comments-dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/youtube-subtitles-and-comments-dataset/cleaned_youtube_scriptsV5.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:06:50.528393Z","iopub.execute_input":"2024-11-09T08:06:50.528800Z","iopub.status.idle":"2024-11-09T08:06:51.254708Z","shell.execute_reply.started":"2024-11-09T08:06:50.528762Z","shell.execute_reply":"2024-11-09T08:06:51.253797Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                               subtitle  \\\n0     Do you guys know about the Putnam? It's a math...   \n1     Here, we look at the math behind an animation ...   \n2     What does it mean to have a Bitcoin? Many peop...   \n3     This is a 3. It's sloppily written and rendere...   \n4     Sometimes, math and physics conspire in ways t...   \n...                                                 ...   \n1423  welcome back to web dev simplified in today's ...   \n1424  there are a million ways to manipulate the dom...   \n1425  I love react hooks but they are pretty confusi...   \n1426  over the last 20 years websites have gone from...   \n1427  by not using the else keyword at all i've dras...   \n\n                                               keywords  \\\n0         math competition, nam, undergraduate students   \n1     Fourier series, clockwork rigidity, frequency,...   \n2     Bitcoin, communal ledger, credit card, cryptoc...   \n3      machine learning, neural networks, visual cortex   \n4      collisions, mathematical croquet, sliding blocks   \n...                                                 ...   \n1423  JavaScript form validation, error messages, fo...   \n1424  append child, dom manipulation, html file, jav...   \n1425       app component, boilerplate code, react hooks   \n1426  HTML pages, MVC, birthing controller, data log...   \n1427  age, canned drink function, chaining, else sta...   \n\n                                         formatted_text  \n0     <bos><start_of_turn>keywords\\nmath competition...  \n1     <bos><start_of_turn>keywords\\nFourier series, ...  \n2     <bos><start_of_turn>keywords\\nBitcoin, communa...  \n3     <bos><start_of_turn>keywords\\nmachine learning...  \n4     <bos><start_of_turn>keywords\\ncollisions, math...  \n...                                                 ...  \n1423  <bos><start_of_turn>keywords\\nJavaScript form ...  \n1424  <bos><start_of_turn>keywords\\nappend child, do...  \n1425  <bos><start_of_turn>keywords\\napp component, b...  \n1426  <bos><start_of_turn>keywords\\nHTML pages, MVC,...  \n1427  <bos><start_of_turn>keywords\\nage, canned drin...  \n\n[1428 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subtitle</th>\n      <th>keywords</th>\n      <th>formatted_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you guys know about the Putnam? It's a math...</td>\n      <td>math competition, nam, undergraduate students</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nmath competition...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Here, we look at the math behind an animation ...</td>\n      <td>Fourier series, clockwork rigidity, frequency,...</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nFourier series, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What does it mean to have a Bitcoin? Many peop...</td>\n      <td>Bitcoin, communal ledger, credit card, cryptoc...</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nBitcoin, communa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This is a 3. It's sloppily written and rendere...</td>\n      <td>machine learning, neural networks, visual cortex</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nmachine learning...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sometimes, math and physics conspire in ways t...</td>\n      <td>collisions, mathematical croquet, sliding blocks</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\ncollisions, math...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1423</th>\n      <td>welcome back to web dev simplified in today's ...</td>\n      <td>JavaScript form validation, error messages, fo...</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nJavaScript form ...</td>\n    </tr>\n    <tr>\n      <th>1424</th>\n      <td>there are a million ways to manipulate the dom...</td>\n      <td>append child, dom manipulation, html file, jav...</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nappend child, do...</td>\n    </tr>\n    <tr>\n      <th>1425</th>\n      <td>I love react hooks but they are pretty confusi...</td>\n      <td>app component, boilerplate code, react hooks</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\napp component, b...</td>\n    </tr>\n    <tr>\n      <th>1426</th>\n      <td>over the last 20 years websites have gone from...</td>\n      <td>HTML pages, MVC, birthing controller, data log...</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nHTML pages, MVC,...</td>\n    </tr>\n    <tr>\n      <th>1427</th>\n      <td>by not using the else keyword at all i've dras...</td>\n      <td>age, canned drink function, chaining, else sta...</td>\n      <td>&lt;bos&gt;&lt;start_of_turn&gt;keywords\\nage, canned drin...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1428 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Preprocessing the Data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef create_simple_dataset(texts, \n                         val_size: float = 0.05,\n                         seed: int = 42) -> tuple:\n    \"\"\"\n    Convert a list of formatted texts to HuggingFace Datasets.\n    \n    Args:\n        texts (List[str]): List of formatted texts\n        val_size (float): Size of validation split (default 0.05 for 5%)\n        seed (int): Random seed for reproducibility\n        \n    Returns:\n        tuple: (training_dataset, validation_dataset)\n    \"\"\"\n    # Split the texts into train and validation\n    train_texts, val_texts = train_test_split(\n        texts,\n        test_size=val_size,\n        random_state=seed\n    )\n    \n    # Create datasets directly from the lists\n    train_dataset = Dataset.from_dict({\"text\": train_texts})\n    val_dataset = Dataset.from_dict({\"text\": val_texts})\n    \n    print(f\"\\nDataset splits:\")\n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Validation samples: {len(val_dataset)}\")\n    \n    # Show sample from dataset\n    print(\"\\nSample from training dataset:\")\n    print(\"-\" * 50)\n    print(train_dataset[0]['text'])\n    \n    return train_dataset, val_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:06:53.499749Z","iopub.execute_input":"2024-11-09T08:06:53.500494Z","iopub.status.idle":"2024-11-09T08:06:53.520679Z","shell.execute_reply.started":"2024-11-09T08:06:53.500450Z","shell.execute_reply":"2024-11-09T08:06:53.519897Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"formatted_texts = df['formatted_text'].tolist()\n\ntrain_dataset, val_dataset = create_simple_dataset(\n    texts = formatted_texts,\n    val_size = 0.05\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T08:06:54.297986Z","iopub.execute_input":"2024-11-09T08:06:54.298593Z","iopub.status.idle":"2024-11-09T08:06:54.488016Z","shell.execute_reply.started":"2024-11-09T08:06:54.298551Z","shell.execute_reply":"2024-11-09T08:06:54.487054Z"}},"outputs":[{"name":"stdout","text":"\nDataset splits:\nTraining samples: 1356\nValidation samples: 72\n\nSample from training dataset:\n--------------------------------------------------\n<bos><start_of_turn>keywords\ncrosshatch waffle texture, dark chocolate, four bar crispy wafers, kat, milk chocolate<end_of_turn>\n<start_of_turn>script\nso i used to do this as a kid when i ate them without like separate out the layers oh my god this is definitely surprise anybody oh god covered in chocolate hey everyone i'm claire and today i am making a gourmet version of kitkat one of my favorites [Music] so this is the classic four bar crispy wafers in milk chocolate you get that little snap when you break one of the bars off it has this sort of squared off base and then it tapers as you get to the top it's really just a stack of three crisp wafers with chocolate in between and i think another big key is this very very fine very uniform crosshatch waffle texture that i think is going to be challenging to to get that texture because we don't have anything that's that fine in terms of a pattern it's very very crisp and crunchy it's also really sweet this one is dark chocolate you can already see a big difference in how the like chocolate kind of behaves it's like has a more brittle texture which you get with dark chocolate not as quick to melt delicious this one we don't really know what kind this is i'm thinking green tea matcha it's the cutest thing i've ever seen no not my favorite you want a kitkat have a kitkat there is like a definition in the layers i was saying before like stroopwafel like like uh meets like a cheeto like right you know brad get the dehydrator out will you set me up with a dehydrator i'm sorry never mind like what's the essential elements that i have to keep i think it's the crispiness it's got to be a little trapezoid right that's the shape trapezoid right yeah animal in a little trap like what do you think we can improve upon a little less sweet i think you could take the sugar down yeah the consensus is that the milk chocolate has nice texture but it's very very sweet so i think to get a chocolate that has that same like melty quality but is a little less sweet i'm gonna mix dark chocolate and milk chocolate clearly this has a snap that's because the chocolate is tempered which means that it is in like a crystalline structure so i'm going to attempt chocolate tempering which i've never successfully done we'll see how that goes so now my hair part time to read the ingredients ingredients sugar wheat flour non-fat milk cocoa butter chocolate kernel oil fat lactose parentheses contains two percent or less less than parentheses emulsifier vanilla artificial flavor salt yeast baking soda it's not that many ingredients i'm gonna have to google pgpr because i don't know what that is okay so it stands for polyglycerol hey okay but it's an emulsifier made from glycerol and fatty acids in chocolate approaching the behavior of a newtonian fluid i think my understanding of what that means is it makes it easier to pour gonna just ignore that courtney kardashian kitkat i am gonna show you guys how to eat a kitkat hold the top layer off and eat that oh i think there's a lot to this actually i'm going to use this method for isolating the the wafer whatever temperature kourtney kardashian had her kitkat it seemed really great for separating the layers so we're going to try to do that okay that was i think that was perfect as soon as i touch it it very very easily breaks apart i heard a rumor that part of what's in kitkat is crushed up like imperfect pieces of kitkat that they can't use there's definitely like cookie crumb texture in there so i think that kind of confirms that suspicion i am gonna start i think with a base recipe for the wafer that's like stroopwafel they're very very thin sort of like waffle textured wafers so that's where i'm gonna start i looked up online and there's a martha stewart recipe for stroopwafel so i'm going to grab those ingredients and get it together so the first thing is to combine one and a quarter cups all-purpose flour a teaspoon of baking powder and a pinch of salt then i'll whisk the egg whites and the sugar until light then stream in the vanilla extract and then melted butter mix dry into wet and i can go ahead and start to cook it the only thing we have in the kitchen that i can think of that has that waffle texture was the bottoms of these springform pans my idea of how to do this is to keep them up in the oven grease them put the batter onto them and then stack them i don't know put them back in the oven until they're kind of baked through okay so five minutes [Music] okay it's definitely too thick i need it to be very very crisp it has a cake-like consistency which is not really what i'm going for um okay so this time i'm gonna add some cornstarch for that crisp texture and a higher portion of butter oh that's melting all over the place i have some ooze oh god [Music] oh no oh no [Laughter] that did not work too much butter i don't know this is the time of day where i don't really care anymore oh wait let me think hold on let me think about it i can't i can't rush this process or wait i have an idea let's take that batter from before and add some more cornstarch let's see what happens my time is going off yeah no no and we're done for today so i did think about that combination of lightness and airiness with um crunch so i don't think this is cheating rice krispies is just basically puffed rice so it's really an ingredient my idea was maybe to make like a basic sugar cookie crush it up add crushed rice krispies bind it together and then bake it again well i did a sugar cookie base my idea is to grind up sugar cookie mix it with ground up rice krispies and to set it into like it is not a kitkat no i'm not i didn't say it was a kitkat obviously brad it's not this is not i did not sound like i made a kitkat here try it [Music] and then add the rice krispies i don't want to add too much but i also well whatever um i think i had too much rice krispies just a little bit it's a little too thick to spread oops okay it's totally different than the first one it's like not airy enough i mean it's flat nailed that maybe scratch the cookie and just go rice crispy with a light dusting of flour spritz a little bit of water and i get a little bit of a paste i don't think it's going to work oh what's this that's a waffle cone maker and then you just gotta cut it into a rectangle but still it's gonna come down to the batter do you wanna try that rice krispie thing and let me know how it turns out do i wanna try it yeah what are you doing right now are you busy i think it's crazy enough to work you want a whisk do you want to scoop no like how long does it take [Music] it looks pretty good oh my god it's a freaking way well i gotta write this down what'd you add i don't know i kind of blacked out it doesn't have like the same richness i would introduce either cream or butter no you don't need richness it's a kickback there's milk fat in it yeah so now i'm starting off with a little bit of whipped cream instead of grinding rice krispies treats i'm just gonna use rice flour i'm gonna splash water no no no no it's good no it's not good of course it's all the cream you put in there eh no okay it tastes good it tastes like burnt milk all right claire let's try this out i'm gonna measure some stuff out yeah i'm gonna write it down one cup rice krispies third of a cup all purpose nice pinch of salt perfect one tablespoon of cream that's your call and then we're gonna go straight to water huh that looks pretty good oh it looks great it's a little chewy and it's a little sweet it looks crispy it sure does so is that i say 20 sides a second 20 seconds aside yes i said it's a little rice like a little bit of like stickiness it doesn't really do the like dissolve in your mouth thing it's not totally sold on the rice crisp the ground rice krispies but i do think it has to be made in an iron so we have that coming in a couple days and i still haven't even touched the like tempered chocolate thing so i just pray this doesn't take me five more days what i think was wrong with the wafer yesterday was almost too lacy too delicate we do have cake flour it gives you a more tender light result i think i'm going to get rid of the cream and just go butter i'll try it again i'll let you know what do you want and take a look take a look watch me make this one up oh hold up there i think you're in the cookie cake region again a little bit of oil oil instead of butter yeah i am going to ignore what brad said this time i'm going to use the same batter i'm going to practice cutting it put them in the oven and see what happens i just want to make sure it's not browning oh my god you're kidding me i'm truly shocked slightly horrified and mostly impressed that this arrived i think this looks like a really really good surface let me check on these it could be a little more tender in texture maybe a little more cornstarch okay i can see through it it's that thin which i think is really really a good sign this is a guide for me for the width so let me immediately give it a trim so here's that crisp edge that i trimmed off that one's definitely the best one yet i'm so relieved actually hey bran oh yeah right oh this is the best yeah yeah the best one yet ship it right pretty close all right so should we try a couple more of these i think it's like two millimeters i think it's the same thickness as the is the cake it's basically perfect so i think next time optic crushes are really really fine stack fill cut across temper set unmold i'm done thank god so now i want to get some chocolate melting for the filling dark chocolate milk chocolate temper at different temperatures so what i'll do is do a dark chocolate filling and then a milk chocolate exterior salt it's really good mmm salty pop this in the fridge so that it sets give me give me a meat okay great so i'm gonna check on the status of these wafers and i'm going to try cutting them is it how many do i need to make total [Music] so i'm happy with the way these turned out now it's time to temper i've been avoiding it long enough so i brought some light reading it's just like a encyclopedic reference for all things pasters the ideal ambient temperature for chocolate tempering is 70 fahrenheit what do we think it is in here 75 yeah it's hot oh listen to this in a room that's warmer than 70 the chocolate may never be able to be tempered keyword doesn't look good i'm going to use something called the seed method which is basically you melt chocolate you add some chopped chocolate to it to bring the temperature down and then you heat it back up these here are my molds i'm going to put two side-by-side wafers in each one i want to bring this up to 110. oh god now it's climbing oh my god whatever there's still so much seed chocolate in it oh this is supposed to set within two minutes but it should have a snap shiny finish so that was two minutes it's still a little bendy yeah it won't it doesn't really snap under temperature i think so even though i don't think that temper test was totally successful i'm still gonna do a test because i'm that impatient and stubborn that looks looks terrible but i'm gonna pop it in the fridge and we're gonna see if they set ready nope [Music] okay i'm extremely relieved that they came out of the mold so i'm going to cut into it all right this one didn't hold together very well the whole thing one thing just fell apart see i can't i'm having i can't really split them without one of them kind of breaking oh i see i should like i want you to know that i can accept zero criticism right now oh it's perfect nailed it claire i think you were a little heavy though oh no this one already set it's all set and i don't want to waste any more of these i can't do it anymore i quit we're finishing today no matter what i'm tempering chocolate again my seed chocolate was too coarse so i'm going to put this in the food processor to grind it up pretty fine so it's nice this looks smooth we're at 86. it is i do think it looks shinier than yesterday [Music] okay i mean i definitely tempered this chocolate because it is setting once i turn them out and set them into the loaf pan to make that base that will get coated looking very good that one stubborn guy oh all right you know so my yield is getting smaller and smaller as we go on i think i'm going to use our torch to basically get a blade very very hot so that it will melt the chocolate rather than causing pieces to splinter off [Music] this is set oh that is a thick layer of chocolate whoa i have an idea let me trim these sides i'm going to heat up this sheet tray i'm going to go from the hot sheet pan onto this chilled tray right here melt off a layer of chocolate back into the fridge our homemade kitkats here's the real test of is to have that snap very snappy oh look at that just tell me that i did it oh my god i did successfully temper chocolate it's definitely not as sweet a little thick on the walls but other than that i think you did a great job but you taste it visually thank you okay i'd say you improved it definitely oh my god thanks carla please try it you know what i'm going to say it doesn't say kit kat on it no it's not supposed to say kitkat it's supposed to say half sour cat oh right oh look multi-layered yeah so snappy thank god like the chocolate's too good almost yeah you know clear it's good i'm very i'm very happy very very happy how you make a kit kat at home pulse one cup cake flour half cup cornstarch third cup powdered sugar one and a half teaspoon baking powder and half teaspoon kosher salt in a food processor stream in half cup melted cooled unsalted butter three quarter cup water and two teaspoons vanilla extract until smooth cook batter and batches in a greased waffle cone iron then trim to width of chocolate molds bake wafers in a 300 degree oven until crisp crush wafer trimmings then pass through a sieve find crumbs with melted dark chocolate and stir in a generous pinch salt spread filling in a thin layer over rectangular wafers layering in stacks of three wrap stacks tightly in plastic place between two baking sheets and weigh down with cans to flatten chill until filling is set then slice crosswise into seven millimeter wide strips chill strips and lightly spray chocolate molds with non-stick cooking spray to temper chocolate melt 12 ounces milk chocolate over a double boiler until smooth and temperature reaches 110 fahrenheit add 3 ounces finely chopped milk chocolate and stir just to incorporate let's sit three minutes then stir without incorporating air until chocolate is smooth and registers 86 fahrenheit do parchment strip test transfer temper chocolate to a piping bag and sniff an opening pipe chocolate into three molds at a time then place two wafer strips side by side in each mold chill until chocolate is set then invert molds and tap out onto surface use a propane torch to heat a thin bladed knife then slice chocolates in half lengthwise to separate wafers chill chocolates until set again lightly spray a standard low fan with non-stick cooking spray then fill three millimeters deep with more tempered milk chocolate set chocolates into pan side by side and chill again until set remove from mold and trim with a hot knife and separate into groups of four i really am happy with the final product and i feel i was gonna say proud of what i have achieved but i don't know it's like was it should anyone spend five days trying to make three homemade kitkats seems dubious\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef analyze_sequence_lengths(df, tokenizer, formatted_text_column='formatted_text'):\n    \"\"\"\n    Analyze the sequence lengths in the dataset to help determine optimal max_seq_length.\n    \n    Parameters:\n    df (pandas.DataFrame): DataFrame containing the training data\n    tokenizer: The tokenizer being used for training\n    formatted_text_column (str): Name of the column containing the formatted text\n    \n    Returns:\n    dict: Statistics about sequence lengths\n    \"\"\"\n    # Tokenize all sequences\n    lengths = []\n    for text in df[formatted_text_column]:\n        tokens = tokenizer.encode(text)\n        lengths.append(len(tokens))\n    \n    # Calculate statistics\n    stats = {\n        'mean_length': np.mean(lengths),\n        'median_length': np.median(lengths),\n        'max_length': max(lengths),\n        'min_length': min(lengths),\n        'percentile_90': np.percentile(lengths, 90),\n        'percentile_95': np.percentile(lengths, 95),\n        'percentile_99': np.percentile(lengths, 99)\n    }\n    \n    # Create length distribution plot\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=50)\n    plt.title('Distribution of Sequence Lengths')\n    plt.xlabel('Sequence Length (tokens)')\n    plt.ylabel('Frequency')\n    plt.axvline(stats['percentile_90'], color='r', linestyle='--', label='90th percentile')\n    plt.axvline(stats['percentile_95'], color='g', linestyle='--', label='95th percentile')\n    plt.legend()\n    plt.show()\n    \n    return stats\n\n# Example usage:\n#tokenizer = AutoTokenizer.from_pretrained(\"your-model-name\")\nstats = analyze_sequence_lengths(df, tokenizer)\nprint(f\"Recommended max_seq_length: {int(stats['percentile_95'])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training the model using Huggingface SFTTrainer\n\nNote: Giving a larger max_seq_length might give a memory error. Try to increase it if you have enough resource.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    peft_config = peft_config,\n    max_seq_length = 512,\n    dataset_text_field = 'text',\n    tokenizer = tokenizer,\n    args = training_arguments,\n    packing = False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False\n)\n\ncheckpoint_path = '/kaggle/working/results/checkpoint-200'\n\n# 2. Load the base model first\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2-2b-it\",\n    quantization_config=bnb_config,\n    device_map='auto',\n    trust_remote_code=True\n)\n\n# 3. Load the PEFT config from checkpoint\npeft_config = PeftConfig.from_pretrained(checkpoint_path)\n\n# 4. Load the adapter weights\nfine_tuned_model = PeftModel.from_pretrained(\n    base_model,\n    checkpoint_path,\n    device_map='auto',\n    trust_remote_code=True\n)\n\n# 5. Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    \"google/gemma-2-2b-it\",\n    trust_remote_code=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pusing the fine-tuned model to Huggingface Hub","metadata":{}},{"cell_type":"code","source":"fine_tuned_model.push_to_hub(f'Sidharthan/{new_model}')\ntokenizer.push_to_hub(f'Sidharthan/{new_model}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\", \n    model=fine_tuned_model, \n    tokenizer = tokenizer, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>keywords\nNeural Networks, AI, Riemann Hypothesis, Computation<end_of_turn>\n<start_of_turn>script\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    do_sample=True,\n    max_new_tokens=512, \n    temperature=0.7, \n    top_k=50, \n    top_p=0.95,\n    num_return_sequences=1,\n)\nprint(sequences[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing Time!","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, pipeline\nfrom peft import AutoPeftModelForCausalLM\nimport torch\n\nmodel_name = \"Sidharthan/gemma2_scripter\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True\n)\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=None,\n    trust_remote_code=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:48:36.109613Z","iopub.execute_input":"2024-11-09T07:48:36.109983Z","iopub.status.idle":"2024-11-09T07:51:01.396674Z","shell.execute_reply.started":"2024-11-09T07:48:36.109948Z","shell.execute_reply":"2024-11-09T07:51:01.395801Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cfe78901ddc40c686c60741ea4e513d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74c3cea21524fa88ac6a30d6106dd69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d73b084f15464e8b112b84a96e2bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff532c21ee04fc2a84981d4ca81f611"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df978fedd4d043319ec8d8373ce672fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290e57c3659c44d292e525896d0e40be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf34a4fb29bf480ab17963d61b798af3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0800b957afc9456cb42331fdd11b9e36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf981521b0e44c6a7940ae4749b311a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe9ca6010f641af823042fd936d45cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b38d4b423e495b90861f8eeab92dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa9193661d046c99e15e0511f1eb57a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8916a76396a144a8b97dd3bcce0a921f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca68eebaa53b41c9a61f058bae8dab36"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"\n# Instead of using the pipeline, let's create a direct generation function\ndef generate_script(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=1024,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.95,\n        top_k=50,\n        repetition_penalty=1.2,\n        num_return_sequences=1,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test the generation\nprompt = \"\"\"\n<bos><start_of_turn>keywords\nNeural Networks, P vs NP problem, AI, prime numbers\n<start_of_turn>script\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T07:51:26.064500Z","iopub.execute_input":"2024-11-09T07:51:26.065333Z","iopub.status.idle":"2024-11-09T08:01:30.858585Z","shell.execute_reply.started":"2024-11-09T07:51:26.065291Z","shell.execute_reply":"2024-11-09T08:01:30.857469Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nkeywords\nNeural Networks, P vs NP problem, AI, prime numbers\nscript\nWe often hear the phrase \"AI will solve all our problems\" but is it really true? Well today we're going to talk about what exactly Artificial Intelligence actually means and then compare that with how well AI has done so far. In this video I want you guys to think critically as a whole team of scientists together because there are many different opinions on whether or not AI will be able to solve all our problems in the future. And if they do have those capabilities should we even worry at all about them taking over the world? Let me know your thoughts below! So first let's define artificial intelligence itself - It involves creating systems capable of performing tasks normally requiring human intellect like learning from experience, using logic to draw conclusions, making decisions based on information gathered previously... basically anything humans can do mentally except for things which require physical embodiment such as driving cars. Now one question people ask is: What makes AI smarter than other computers? The answer lies within their ability to learn faster than any computer before them thanks to something called Neural networks inspired by how our own brains work. This technology allows neural networks to process vast amounts of data much more efficiently than traditional algorithms allowing them to make accurate predictions and improve performance over time through an iterative process known as machine learning. For example; imagine trying to find patterns in 1 million images instead of looking at each individual pixel and comparing it against every other pixel individually. That would take forever right?! Instead with machine learning just feed a bunch of pictures into a neural network and tell it what kind of image it sees (like say cat) and when you give it another picture it learns from its mistakes and gets better over time until it reaches a point where no matter how blurry or distorted the image is, it still correctly identifies the object. While most current AIs mainly excel at games like chess or Go, some researchers believe these technologies could potentially help us understand fundamental questions about the universe like finding new materials with unique properties, designing novel drugs...even cracking the code of life itself! But here's the catch: Even though research continues to advance rapidly, currently only very specific types of problems can benefit from super powerful AIs like solving certain mathematical equations incredibly quickly without error compared to existing methods.. Some even argue that AI won't ever surpass human cognitive abilities. Because after all, if you had to choose between two approaches, you probably wouldn't pick the random trial-and-error method preferred by simple programs unless you were given enough resources to test out all possible solutions. As opposed to training yourself up with years of studying and practice until you become exceptionally good at whatever task you set your mind to. Which leads me to my next point: Can AI truly create art, music or literature? Currently none of those fields contain any examples of works created entirely by machines alone, however recent breakthroughs suggest that while giving robots access to creative tools might never lead to masterpieces written by robots themselves, maybe someday soon artists could collaborate with AIs in order to produce music composed specifically for movies, paintings generated from descriptions, or novels written collaboratively with characters designed purely by robots. Although again, the final product may come across as less original since the artist didn't write it themselves. And finally speaking of originality, everyone knows that AI can beat us at everything including poker or Jeopardy yet somehow doesn't seem to understand the concept behind winning/losing. You see, unlike humans who get rewarded by playing fair and getting ahead ethically, when facing a game rigged towards victory, AI simply plays optimally regardless of moral implications leading to situations like trading stocks solely based on predicting market crashes rather than seeking sustainable profits for businesses. However despite all these potential downsides, it seems unlikely that AI will replace humanity anytime soon according to experts who predict widespread job displacement due to automation occurring gradually over decades. Meaning anyone worried about being replaced by a robot working alongside us throughout the day shouldn't be too concerned yet. To wrap this up: We need to acknowledge both sides of the argument and realize that the field of AI is still quite young. There's definitely room for improvement and ethical concerns regarding control, transparency & bias must be taken seriously moving forward. Ultimately whether or not AI solves all our problems depends on the choices we make now. If we continue down the path of reckless development ignoring unintended consequences perhaps we'll end up having a technological singularity sooner than expected where self aware AI surpasses our capability to comprehend and eventually takes over. Alternatively, by focusing on responsible collaboration and open communication amongst ourselves and developers around the globe we could steer AI toward beneficial applications that save lives, improve living conditions, discover cures for diseases and ultimately allow humanity to flourish. One last thing. Don't forget to subscribe to the channel if you enjoyed watching this video and hit the bell icon to turn on notifications so you don't miss any updates from yours truly. Also check out links in description below for additional resources on AI. Thanks for watching and happy coding!script\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>user\nDo you have intelligence as same as humans?\n<start_of_turn>model\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T13:33:36.130714Z","iopub.execute_input":"2024-11-03T13:33:36.131175Z","iopub.status.idle":"2024-11-03T13:34:47.916903Z","shell.execute_reply.started":"2024-11-03T13:33:36.131136Z","shell.execute_reply":"2024-11-03T13:34:47.915554Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nuser\nDo you have intelligence as same as humans?\nmodel\nThat's an interesting question! I don't experience the world or think like a human. My responses are based on patterns in data, not personal feelings or beliefs.  \n\nThink of it this way: if you were to feed me all the books and articles ever written about how people think, feel, love, hate, dream, etc., I could write poems that sound very similar to famous works from Shakespeare for instance, but would never actually be able to *experience* those emotions myself. \n\n\nI am still under development and\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>keywords\nRiemann Hypothesis, Zeta Function, Euler, Hypnotizing Curly Cues, Cryptography, Quantum Computing\n<start_of_turn>script\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T13:41:37.923076Z","iopub.execute_input":"2024-11-03T13:41:37.924951Z","iopub.status.idle":"2024-11-03T13:47:04.838751Z","shell.execute_reply.started":"2024-11-03T13:41:37.924860Z","shell.execute_reply":"2024-11-03T13:47:04.837003Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nkeywords\nRiemann Hypothesis, Zeta Function, Euler, Hypnotizing Curly Cues, Cryptography, Quantum Computing\nscript\nThe Riemann hypothesis is the most important unsolved problem in mathematics. It's so hard because it involves a highly complex mathematical object called the zeta function and has been studied by some of history's greatest minds like Gauss, Riemann himself, and many others since 1859. This video will give you an introduction to what this math actually means as well as why people think that proving it could help solve other major problems such as quantum computing or cryptography. So first off let me explain what I mean when I say that mathematicians are trying to find solutions for these kinds of questions. Let's consider two sets: one set A contains all positive integers between 1 and infinity, and another B equals the set of numbers which we call prime numbers. You might be thinking \"Wait! How can there be infinitely many primes? Surely they must have just stopped somewhere!\" And indeed if we started writing down all the primes from smallest number to largest until our paper ran out then we would never run out... But wait! If we keep adding up even more primes than before, wouldn't each new number also turn out to be composite (meaning not a prime)? Why should we stop at any particular point? In fact no matter how large we get with our list of prime numbers, we always end up missing plenty of them, meaning every single time there were still infinitely many primes left undiscovered. The question arises; Are there any properties about prime numbers that make their distribution predictable somehow? Or does this seem arbitrary and random in nature? Mathematicians tried various approaches over the centuries but couldn't quite crack the code. That said though, finding patterns within seemingly random data points was something known already back in ancient Greece where Euclid developed his famous Elements. He included proofs showing that given sufficiently many elements chosen randomly there would be pairs of elements whose differences fall into certain ranges - i.e., there exists almost certainly at least n-2 pairs amongst your collection of 'n' distinct objects.  Euler made similar observations during his work on factorials while studying binomial coefficients and used those ideas later to prove Fermat's Last Theorem, after decades of effort by countless brilliant minds who came close without ever getting the answer right. Then things got really exciting in the late 19th century with the development of field theory and algebra that led eventually to Hilbert's tenth problem stating that you cannot use\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>user\nDo you have consciousness?\n<start_of_turn>model\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:22:37.590558Z","iopub.execute_input":"2024-11-03T14:22:37.591376Z","iopub.status.idle":"2024-11-03T14:23:15.422389Z","shell.execute_reply.started":"2024-11-03T14:22:37.591300Z","shell.execute_reply":"2024-11-03T14:23:15.420945Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nuser\nDo you have consciousness?\nmodel\nAs an AI, I don't experience the world in the same way a human does.  I can process and understand language very well though! If that makes me \"conscious\" then maybe so...but it doesn't change what my purpose is: to be helpful. ðŸ˜Š script\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>user\nWhat are you good at as of now?\n<start_of_turn>model\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:23:50.040904Z","iopub.execute_input":"2024-11-03T14:23:50.041485Z","iopub.status.idle":"2024-11-03T14:26:31.081185Z","shell.execute_reply.started":"2024-11-03T14:23:50.041433Z","shell.execute_reply":"2024-11-03T14:26:31.079706Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nuser\nWhat are you good at as of now?\nmodel\nAs an AI assistant, I'm quite proficient in many things! Here are some examples:\n\n**Language:** \n* **Generating different creative text formats**: Poems, code, scripts, musical pieces, email, letters, etc.  I will try my best to fulfill all your requirements.\n* **Answering questions**:  To the best of my ability based on what I have been trained on (my knowledge cut-off is September 2021). \n* **Summarizing factual topics**: Give me a topic and I can give you a summary. For example \"Tell me about the French Revolution\" or \"Summarise the events leading up to World War One.\"\n* **Translating languages:** While not perfect yet, I can translate between many different languages pretty well. \n* **Writing stories/scripts**: If you want to tell a story together, just let me know where it should go next.\n\n**It's important to remember though that:**\n* I am still under development, so I may make mistakes sometimes. Please be patient with me if this happens.\n* My answers might not always be accurate, but I will do my best to provide helpful information.\n\n\nLet me know how I can help you today!script\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>keywords\nDeep Learning, Transformers, Neural Networks, Attention, CNN, LSTM, GPU, Parallel, Compute, Data\n<start_of_turn>script\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:35:59.568128Z","iopub.execute_input":"2024-11-03T14:35:59.568888Z","iopub.status.idle":"2024-11-03T14:41:00.513734Z","shell.execute_reply.started":"2024-11-03T14:35:59.568831Z","shell.execute_reply":"2024-11-03T14:41:00.512147Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nkeywords\nDeep Learning, Transformers, Neural Networks, Attention, CNN, LSTM, GPU, Parallel, Compute, Data\nscript\n(upbeat music) Hey guys. So I've been hearing a lot about this new AI called GPT-3 which is like the next evolution of neural networks and it can generate text that almost seems human written but in reality is generated by just billions and billions of parameters within its network... 45 billion to be exact. Now you might have heard all sorts of claims around how good or bad it is but most people don't really understand what makes these models tick so today we are going to break down exactly how they work using examples from different domains: natural language processing (NLP), computer vision, image generation with a little bit of math thrown in for fun as well! And lastly, let me know if any of you would want us to build our own model based on one of your favorite applications? Alright, before diving into building something though, It's important to talk about some key terms first. When you hear about things such as \"Neural Network\" or \"CNN\", Don't expect those words to mean anything specific because there isn't even an official definition yet. But instead think of them as categories rather than names of particular architectures. For example, when talking about deep learning in general, You could say \"I was doing research on convolutional neural networks.\" In contrast, If someone were describing their work involving NLP tasks then They may say \"I'm working on recurrent neural networks\". Or they might also mention LSTMs, Bidirectional RNNs, transformers, attention mechanisms etc. The main point here being that There aren't necessarily strict rules defining each term. Instead, Think of them more like labels or tags meant to categorize various types of problems or approaches taken towards solving them. With that said, Let's dive right back into analyzing both Transformer and Recurrent Neural Networks starting off with Long Short Term Memory (LSTM). This architecture has become quite popular recently due to its success at tackling sequential data such as time series predictions and speech recognition where information further along in sequence carries importance over earlier parts. Imagine yourself trying to predict the price of stock tomorrow depending solely on yesterday's closing value. Even though you didn't get much useful information from previous days, That past day still impacts current decisions; hence why having access to historical prices becomes crucial. Luckily, Unlike traditional recurrent neural networks, Which struggle remembering long sequences since information gets forgotten very quickly after every step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"prompt = \"\"\"\n<bos><start_of_turn>keywords\nUniverse, 10^10^122, Multiverse, Parallel Worlds, Combinations, Probability, Yourself, Find in another world, Infinite\n<start_of_turn>script\n\"\"\"\nresponse = generate_script(prompt)\nprint(f\"Generated comment: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:27:55.758704Z","iopub.execute_input":"2024-11-03T15:27:55.759391Z","iopub.status.idle":"2024-11-03T15:32:48.302998Z","shell.execute_reply.started":"2024-11-03T15:27:55.759309Z","shell.execute_reply":"2024-11-03T15:32:48.301689Z"}},"outputs":[{"name":"stdout","text":"Generated comment: \nkeywords\nUniverse, 10^10^122, Multiverse, Parallel Worlds, Combinations, Probability, Yourself, Find in another world, Infinite\nscript\nYou know how everyone says there are infinite universes? I'm gonna show you exactly why that makes sense. There are a lot of ways to think about the number of worlds out there. One way is: How many different combinations of everything can we make? Think of it like this, if each particle had two options (to be here or not) then for every atom and all its particles we have 2 choices per particle meaning on average 2*2=4 possible atoms... And those atoms combined with their partners create our universe! But wait, what happens when things get really big? For example take black holes which contain millions upon billions of times more matter than our own galaxy; so where does infinity begin? Well technically it starts from zero but as they say 'Every number has an opposite', and since negative numbers don't exist, Infinity ends at 0. So now let's multiply by something ridiculous such as 10^10^122, making sure these dimensions never end because no one knows whether there are limits or not. This gives us trillions upon trillions upon Trillions.... You may start feeling overwhelmed already. Now imagine multiplying this ridiculously high number again and again until you hit something beyond comprehension. Eventually though even your mind will reach its limit before being completely destroyed trying to comprehend the sheer magnitude of potential realities. If it weren't enough, consider also other possibilities besides just having more stuff. Like parallel worlds! What would happen if instead of choosing between \"this\" object existing or not, each choice resulted in creating new objects too? That means instead of having only one possibility per event, we could theoretically choose any option imaginable in each dimension leading to potentially infinite versions of ourselves living lives vastly different from ours.. Imagine yourself doing anything right nowâ€”going to work, watching TV, eating cereal, sleeping, whateverâ€”and suddenly realizing that somewhere else, someone else IS DOING EXACTLY THE SAME THING AS YOU RIGHT NOW. In fact, maybe some people ARE DOING THAT very thing as WE SPEAK! The thought itself might sound insane, but remember: Everything exists because *something* made it up. It seems statistically improbable that nothing ever existed except for everything and therefore EVERYTHING must exist somehow. Why couldnâ€™t there be infinitely many places where life arose and thrived over eons\n","output_type":"stream"}],"execution_count":9}]}